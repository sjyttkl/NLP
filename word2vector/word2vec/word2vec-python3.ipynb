{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec python3 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf8 -*-\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import wget\n",
    "import os\n",
    "import zipfile\n",
    "import heapq\n",
    "import time\n",
    "import itertools\n",
    "import threading\n",
    "import numpy.random as random\n",
    "import sys\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import math\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from queue import Queue\n",
    "from numpy import float32 as REAL\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(threadName)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('word2vec')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Introduction\n",
    "\n",
    "A simple code version for word2vec\n",
    "\n",
    "# Reference\n",
    "[1] https://www.cnblogs.com/pinard/p/7243513.html\n",
    "[2] http://www.cnblogs.com/pinard/p/7249903.html\n",
    "[3] https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py\n",
    "[4] https://github.com/klb3713/word2vec/blob/master/python/word2vec.py\n",
    "\"\"\"\n",
    "# ----- word2vec -----\n",
    "# gensim里用的cython版本。\n",
    "# 本来用cython版本的train_sentence会快很多，不过主要介绍原理，所以都用python介绍。\n",
    "# Word2Vec类中使用的多线程可以在一定程度上加快速度。\n",
    "def train_sentence_sg(model, sentence, alpha, work=None):\n",
    "    \"\"\"\n",
    "    skip-gram\n",
    "\n",
    "    `model` 训练的词向量模型\n",
    "    `sentence` 一个词列表，表示一个句子\n",
    "    `alpha` 学习率\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    count = 1\n",
    "    # 遍历每个句子的词，该词`word`作为中心词\n",
    "    for pos, word in enumerate(sentence):\n",
    "        if word is None:\n",
    "            # 跳过OOV的词\n",
    "            continue\n",
    "        # 设定一个随机的窗口大小，最终真正的单侧窗口大小为window - reduced_window\n",
    "        reduced_window = model.random.randint(model.window)\n",
    "\n",
    "        # 遍历两侧词窗内的所有词，分别预测\n",
    "        start = max(0, pos - model.window + reduced_window)\n",
    "        for pos2, word2 in enumerate(sentence[start : pos + model.window + 1 - reduced_window], start):\n",
    "            if pos2 == pos or word2 is None:\n",
    "                # 跳过OOV的词以及中心词`word`\n",
    "                continue\n",
    "            # 得到输入词向量v_w\n",
    "            # 此处本应该是`word`而不是`word2`，即求P(word2|word)的极大似然，\n",
    "            # 不过这样的话，输入的投影矩阵syn0在每个词窗下只能更新一个词多次，\n",
    "            # 因此此处改成求对称的P(word|word2)的极大似然，这样可以让输入的投影矩阵syn0在每个词窗下更新多个词\n",
    "            l1 = model.syn0[word2.index]\n",
    "\n",
    "\n",
    "            if model.hs >= 1:\n",
    "                # `outer((1 - word.code - fa), l1)`为关于l2a的梯度\n",
    "                # `dot((1 - word.code - fa), l2a)`为关于l1的梯度\n",
    "\n",
    "                # `word.point`是一个array，表示抽取出所有的当前叶子结点的结点路径\n",
    "                # `l2a`是一个矩阵，shape为(codelen, layer1_size)\n",
    "                l2a = model.syn1[word.point]\n",
    "                # 隐层输出，`fa`的shape为(1, codelen)，每个值表示code预测为1（向树右侧走）的概率\n",
    "                fa = expit(np.dot(l1, l2a.T))\n",
    "                # shape为(1, codelen)\n",
    "                ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "                # 更新输出矩阵，shape为(codelen, layer1_size)\n",
    "                model.syn1[word.point] += np.outer(ga, l1)  # learn hidden -> output\n",
    "\n",
    "                # 更新输入矩阵\n",
    "                l1 += np.dot(ga, l2a)\n",
    "\n",
    "                # 计算loss\n",
    "                sgn = (-1.0)**word.code  # ch function, 0-> 1, 1 -> -1\n",
    "                loss = sum(-np.log(expit(sgn * np.dot(l1, l2a.T))))\n",
    "                total_loss += loss / len(word.code)\n",
    "                count += 1\n",
    "                model.running_training_loss += loss\n",
    "            else:\n",
    "                # `outer((model.neg_labels - fb), l1)`为关于l2a的梯度\n",
    "                # `dot((model.neg_labels - fb), l2a)`为关于l1的梯度\n",
    "\n",
    "                # 从构建的`cum_table`中找到`negative`个负样本\n",
    "                neg_indices = [word.index]\n",
    "                while len(neg_indices) < model.negative + 1:\n",
    "                    w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))\n",
    "                    if w != word.index:\n",
    "                        neg_indices.append(w)\n",
    "                # `l2b`是一个矩阵，shape为(negative+1, layer1_size)\n",
    "                l2b = model.syn1neg[neg_indices]\n",
    "                # shape为(1, negative+1)\n",
    "                prod_term = np.dot(l1, l2b.T)\n",
    "                # shape为(1, negative+1)\n",
    "                fb = expit(prod_term)  # propagate hidden -> output\n",
    "                # shape为(1, negative+1)\n",
    "                gb = (model.neg_labels - fb) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "                # 更新输出矩阵，shape为(negative+1, layer1_size)\n",
    "                model.syn1neg[neg_indices] += np.outer(gb, l1)  # learn hidden -> output\n",
    "\n",
    "                # 更新输入矩阵\n",
    "                l1 += np.dot(gb, l2b)  # save error\n",
    "\n",
    "                # 计算loss\n",
    "                loss1 = -sum(np.log(expit(-1 * prod_term[1:]))) \n",
    "                loss2 = -np.log(expit(prod_term[0]))\n",
    "                total_loss += (loss1 + loss2) / len(prod_term)\n",
    "                count += 1\n",
    "                model.running_training_loss += loss1  # for the sampled words\n",
    "                model.running_training_loss += loss2  # for the output word\n",
    "\n",
    "    # 返回句子中的非OOV的词的个数\n",
    "    return len([word for word in sentence if word is not None]), 1.0 * total_loss / count\n",
    "\n",
    "\n",
    "def train_sentence_cbow(model, sentence, alpha, work=None, cbow_mean=True):\n",
    "    \"\"\"\n",
    "    cbow\n",
    "\n",
    "    `model` 训练的词向量模型\n",
    "    `sentence` 一个词列表，表示一个句子\n",
    "    `alpha` 学习率\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    count = 1\n",
    "    for pos, word in enumerate(sentence):\n",
    "        if word is None:\n",
    "            continue\n",
    "        reduced_window = model.random.randint(model.window)\n",
    "        start = max(0, pos - model.window + reduced_window)\n",
    "        window_pos = enumerate(sentence[start:(pos + model.window + 1 - reduced_window)], start)\n",
    "        word2_indices = [word2.index for pos2, word2 in window_pos if (word2 is not None and pos2 != pos)]\n",
    "        # (1, layer1_size)\n",
    "        l1 = np.sum(model.syn0[word2_indices], axis=0)\n",
    "        if word2_indices and cbow_mean:\n",
    "            l1 /= len(word2_indices)\n",
    "\n",
    "        if model.hs >= 1:\n",
    "            # `outer((1 - word.code - fa), l1)`为关于l2a的梯度\n",
    "            # `dot((1 - word.code - fa), l2a)`为关于l1的梯度\n",
    "\n",
    "            # shape为(codelen, layer1_size)\n",
    "            l2a = model.syn1[word.point]\n",
    "            # shape为(1, codelen)\n",
    "            fa = expit(np.dot(l1, l2a.T))\n",
    "            # shape为(1, codelen)\n",
    "            ga = (1 - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "            # 更新输出矩阵，shape为(codelen, layer1_size)\n",
    "            model.syn1[word.point] += np.outer(ga, l1)  # learn hidden -> output\n",
    "\n",
    "            # 更新输入矩阵\n",
    "            if word2_indices and cbow_mean:\n",
    "                neu1e = np.dot(ga, l2a) / len(word2_indices)\n",
    "            else:\n",
    "                neu1e = np.dot(ga, l2a)\n",
    "            for i in word2_indices:\n",
    "                model.syn0[i] += neu1e\n",
    "\n",
    "            # 计算loss\n",
    "            sgn = (-1.0)**word.code  # ch function, 0-> 1, 1 -> -1\n",
    "            loss = sum(-np.log(expit(sgn * np.dot(l1, l2a.T))))\n",
    "            total_loss += loss / len(word.code)\n",
    "            count += 1\n",
    "            model.running_training_loss += loss\n",
    "        else:\n",
    "            # `outer((model.neg_labels - fb), l1)`为关于l2a的梯度\n",
    "            # `dot((model.neg_labels - fb), l2a)`为关于l1的梯度\n",
    "\n",
    "            # 从构建的`cum_table`中找到`negative`个负样本\n",
    "            neg_indices = [word.index]\n",
    "            while len(neg_indices) < model.negative + 1:\n",
    "                w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))\n",
    "                if w != word.index:\n",
    "                    neg_indices.append(w)\n",
    "            # `l2b`是一个矩阵，shape为(negative+1, layer1_size)\n",
    "            l2b = model.syn1neg[neg_indices]\n",
    "            # shape为(1, negative+1)\n",
    "            prod_term = np.dot(l1, l2b.T)\n",
    "            # shape为(1, negative+1)\n",
    "            fb = expit(prod_term)  # propagate hidden -> output\n",
    "            # shape为(1, negative+1)\n",
    "            gb = (model.neg_labels - fb) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "            # 更新输出矩阵，shape为(negative+1, layer1_size)\n",
    "            model.syn1neg[neg_indices] += np.outer(gb, l1)  # learn hidden -> output\n",
    "\n",
    "            # 更新输入矩阵\n",
    "            if word2_indices and cbow_mean:\n",
    "                neu1e = np.dot(gb, l2b) / len(word2_indices)\n",
    "            else:\n",
    "                neu1e = np.dot(gb, l2b)\n",
    "            for i in word2_indices:\n",
    "                model.syn0[i] += neu1e\n",
    "\n",
    "            # 计算loss\n",
    "            loss1 = -sum(np.log(expit(-1 * prod_term[1:])))\n",
    "            loss2 = -np.log(expit(prod_term[0]))\n",
    "            total_loss += (loss1 + loss2) / len(prod_term)\n",
    "            count += 1\n",
    "            model.running_training_loss += loss1  # for the sampled words\n",
    "            model.running_training_loss += loss2  # for the output word\n",
    "\n",
    "\n",
    "    return len([word for word in sentence if word is not None]), 1.0 * total_loss / count\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "        用来存储词汇，如果用hs，则可以看成一个树结点\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        `count` 词频\n",
    "        `index` 索引\n",
    "        `left` 如果用hs，则表示左孩子\n",
    "        `right` 如果用hs，则表示右孩子\n",
    "        `code` 叶子结点的编码路径（从根结点到叶子结点的huffman编码）\n",
    "        `point` 叶子结点的结点路径（从根结点到叶子结点经过的结点索引）\n",
    "        \"\"\"\n",
    "        self.count = 0\n",
    "        self.index = -1\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.code = []\n",
    "        self.point = []\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __lt__(self, vocab):\n",
    "        return self.count < vocab.count\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<' + ', '.join([ '{}:{}'.format(\n",
    "            (_, self.__dict__[_]) for _ in self.__dict__\n",
    "            if not self.__dict__[_].startswith('_'))]) + '>'\n",
    "\n",
    "\n",
    "class Word2Vec(SaveAndLoad):\n",
    "    def __init__(self, sentences=None, size=100, alpha=0.025, \n",
    "                 window=5, min_count=5, seed=1, iters=5, \n",
    "                 workers=1, min_alpha=0.0001, hs=0, sg=0, negative=10, \n",
    "                 sort_vocab=True):\n",
    "        \"\"\"\n",
    "        Word2Vec模型，其中sentences可以不给定，表示不训练模型。\n",
    "        此时初始化的模型可以进行加载模型等操作\n",
    "\n",
    "        `sentences` 输入的用于训练的句子集合，可以是迭代器，也可以是一般的list等\n",
    "        `size` 词向量维度，也就是训练的时候隐变量的维度\n",
    "        `window` 一句话中的当前词与上下文词的最大距离，指单侧词窗\n",
    "        `alpha` 初始学习率，学习过程中随着呈线性下降趋势\n",
    "        `seed` 随机数种子\n",
    "        `iters` 迭代次数\n",
    "        `min_count` 用于过滤的最小词频\n",
    "        `workers` CPU多核的时候可以使用多线程来加快训练\n",
    "        `min_alpha` 最小学习率\n",
    "        `hs` 是否使用hierarchical softmax，当hs>=1时表示使用hs，否则为negative sampling\n",
    "        `sg` 是否使用skip-gram，如果sg>=1表示使用sg，否则为cbow\n",
    "        `negative` 表示负样本个数，用于hs<=0的情况\n",
    "        `sort_vocab` 表示是否对id2word按词频从高到低排序\n",
    "\n",
    "        \"\"\"\n",
    "        self.window = int(window)\n",
    "        self.size = int(size)\n",
    "        self.min_count = int(min_count)\n",
    "        self.alpha = alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.epochs = iters\n",
    "        self.seed = seed\n",
    "        self.random = random.RandomState(seed)\n",
    "        self.workers = int(workers)\n",
    "        self.hs = int(hs)\n",
    "        self.sg = int(sg)\n",
    "        self.negative = int(negative)\n",
    "        self.sort_vocab = sort_vocab\n",
    "        self.cum_table = None # negative sampling时才用到\n",
    "        self.vocab = {}\n",
    "        self.id2word = []\n",
    "        self.syn0 = [] # 即所学的词向量\n",
    "        self.syn1 = [] # 用于hs\n",
    "        self.syn1neg = [] # 用于负采样\n",
    "        self.running_training_loss = 0.\n",
    "        self.layer1_size = self.size\n",
    "        if sentences is not None:\n",
    "            self.build_vocab(sentences)\n",
    "            if self.epochs is not None and self.epochs > 0:\n",
    "                # 重复n次corpus语料\n",
    "                sentences = RepeatCorpusNTimes(sentences, self.epochs)\n",
    "            self.train(sentences)\n",
    "\n",
    "\n",
    "    def get_latest_training_loss(self):\n",
    "        return self.running_training_loss\n",
    "\n",
    "\n",
    "    def __reset_vocab(self):\n",
    "        \"\"\"\n",
    "        重置词典和词表\n",
    "        \"\"\"\n",
    "        self.vocab = {}\n",
    "        self.word_list = []\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"\n",
    "        构建词典，筛除低频词，并给每个词赋予索引index\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        vocab = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                vocab.setdefault(word, Vocab(count=0))\n",
    "                vocab[word].count += 1\n",
    "\n",
    "        # 重置词典\n",
    "        self.__reset_vocab()\n",
    "        # 构建词典\n",
    "        for word, v in vocab.items():\n",
    "            if v.count >= self.min_count:\n",
    "                v.index = len(self.vocab)\n",
    "                self.vocab[word] = v\n",
    "                self.id2word.append(word)\n",
    "        if self.sort_vocab:\n",
    "            self.id2word = list(sorted(self.id2word, key=lambda x: self.vocab[x].count, reverse=True))\n",
    "            for i, word in enumerate(self.id2word):\n",
    "                self.vocab[word].index = i\n",
    "        if self.hs >= 1:\n",
    "            self.create_binary_tree()\n",
    "        else:\n",
    "            self.make_cum_table()\n",
    "        self.reset_weights()\n",
    "\n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        初始化（重置）两个投影矩阵。\n",
    "\n",
    "        syn0用于输入的词向量矩阵，\n",
    "        syn1在输出为hs的时候为tree中每个内部结点\n",
    "        （包括根结点共len(vocab)-1个）的向量矩阵。\n",
    "        \"\"\"\n",
    "        self.syn0 = zeros_aligned((len(self.vocab), self.layer1_size), dtype=REAL)\n",
    "        self.syn0 += (self.random.rand(len(self.vocab), self.layer1_size) - 0.5) / self.layer1_size\n",
    "        if self.hs >= 1:\n",
    "            # syn1的索引对应的不是词，而是huffman树内部结点\n",
    "            # 因此syn0与syn1不能直接结合\n",
    "            self.syn1 = zeros_aligned((len(self.vocab), self.layer1_size), dtype=REAL)\n",
    "        else:\n",
    "            # syn0与syn1neg的索引对应的词一致，可以直接结合（如相加求平均）\n",
    "            self.syn1neg = zeros_aligned((len(self.vocab), self.layer1_size), dtype=REAL)\n",
    "        self.syn0norm = None\n",
    "\n",
    "\n",
    "    def make_cum_table(self, power=0.75, domain=2**31 - 1):\n",
    "        \"\"\"\n",
    "        构建用于negative sampling的cumulative table\n",
    "\n",
    "        `power` 表示压缩值（论文中的值），可以有效防止负采样时候的长尾效应\n",
    "        `domain` 表示采样点的范围，如果归一化到概率，则是[0, 1]，用于轮盘采样\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.id2word)\n",
    "        self.cum_table = np.zeros(vocab_size, dtype=np.uint32)\n",
    "        # compute sum of all power (Z in paper)\n",
    "        train_words_pow = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            train_words_pow += self.vocab[self.id2word[word_index]].count ** power\n",
    "        cumulative = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            cumulative += self.vocab[self.id2word[word_index]].count ** power\n",
    "            self.cum_table[word_index] = round(cumulative / train_words_pow * domain)\n",
    "        if len(self.cum_table) > 0:\n",
    "            assert self.cum_table[-1] == domain\n",
    "\n",
    "\n",
    "    def create_binary_tree(self):\n",
    "        \"\"\"\n",
    "            构建huffman树，用于hierarchical softmax\n",
    "            叶子结点表示词汇。\n",
    "            内部结点包含该路径下的词频和与索引（该索引对应矩阵syn1）\n",
    "            叶子结点包含对应词汇的词频与索引（该索引对应矩阵syn0）\n",
    "            高频词出现在较短的路径上。\n",
    "            叶子结点共`vocab_size`个，而内部结点（包含root结点）共`vocab_size-1`个\n",
    "        \"\"\"\n",
    "        heap = list(self.vocab.values())\n",
    "        print(heap, type(heap))\n",
    "        heapq.heapify(heap)\n",
    "        # 构建huffman二叉树，每次取词频最高的结点构成新结点\n",
    "        for idx in range(len(self.vocab) - 1):\n",
    "            min1, min2 = heapq.heappop(heap), heapq.heappop(heap)\n",
    "            heapq.heappush(heap, Vocab(count=min1.count + min2.count, \n",
    "                      index=idx + len(self.vocab), left=min1, right=min2))\n",
    "        # 此时heap里只有一个结点，即根结点\n",
    "        if heap:\n",
    "            # max_depth记录树最大深度\n",
    "            # stack中三个元素分别代表：当前结点、当前结点的编码路径、当前结点的结点路径\n",
    "            max_depth, stack = 0, [(heap[0], [], [])]\n",
    "            # list可以当做stack用\n",
    "            while stack:\n",
    "                node, codes, points = stack.pop()\n",
    "                # 表示一个叶子结点\n",
    "                if node.index < len(self.vocab):\n",
    "                    node.code = codes\n",
    "                    node.point = points\n",
    "                    max_depth = max(len(codes), max_depth)\n",
    "                else:\n",
    "                    # 表示一个内部结点\n",
    "                    # 编码路径由于是二进制，可以用unit8节省内存\n",
    "                    # 这里讲内部结点的index减去一个vocab的大小，是为了保持内部结点的索引对应矩阵syn1的索引（从0开始）\n",
    "                    points = np.asarray(list(points) + [node.index - len(self.vocab)], dtype=np.uint32)\n",
    "                    stack.append((node.left, np.asarray(list(codes) + [0], dtype=np.uint8), points))\n",
    "                    stack.append((node.right, np.asarray(list(codes) + [1], dtype=np.uint8), points))\n",
    "        logger.info(\"built huffman tree with maximum node depth %i\" % max_depth)\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "\n",
    "    def train(self, sentences, total_words=None, word_count=0, chunksize=100):\n",
    "        \"\"\"\n",
    "        Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
    "        Each sentence must be a list of utf8 strings.\n",
    "        更新词向量权重\n",
    "        每输入一个句子，表示一次迭代更新。\n",
    "\n",
    "        `sentences` 输入的用于训练的句子集合\n",
    "        `total_words` 总词频和\n",
    "        `word_count` 词种类数\n",
    "        `chunksize` 多线程的时候，每个线程一次性处理或被分配到的句子数\n",
    "        \"\"\"\n",
    "        logger.info(\"training model with %i workers on %i vocabulary and %i features\" % (self.workers, len(self.vocab), self.layer1_size))\n",
    "\n",
    "        if not self.vocab:\n",
    "            raise RuntimeError(\"you must first build vocabulary before training the model\")\n",
    "\n",
    "        self.neg_labels = []\n",
    "        if self.negative >= 1:\n",
    "            # 负样本标签\n",
    "            self.neg_labels = np.zeros(self.negative + 1)\n",
    "            self.neg_labels[0] = 1.\n",
    "            print(\"self.neg_labels >>:\", self.neg_labels)\n",
    "\n",
    "        # 用来记录时间的起始与打log的判定时间，\n",
    "        # `next_report`存到list是因为需要在多个线程内作为类对象被访问，`word_count`同理\n",
    "        start, next_report = time.time(), [1.0]\n",
    "        word_count, total_words = [word_count], total_words or sum(v.count for v in self.vocab.values())\n",
    "        print(\"word_count:\", word_count)\n",
    "        print(\"total_words:\", total_words)\n",
    "        # 考虑缓冲区\n",
    "        jobs = Queue(maxsize=2 * self.workers)\n",
    "        # 因为有共享变量（如`word_count`），所以加锁\n",
    "        lock = threading.Lock()\n",
    "\n",
    "        def worker_train():\n",
    "            \"\"\"Train the model, lifting lists of sentences from the jobs queue.\"\"\"\n",
    "            # 多线程配给的memory\n",
    "            work = zeros_aligned(self.layer1_size, dtype=REAL)\n",
    "\n",
    "            while True:\n",
    "                # 一个job就是一个chunksize条句子的数据集\n",
    "                job = jobs.get()\n",
    "                if job is None:  # 数据读完，退出\n",
    "                    break\n",
    "                # 在开始训练之前先减小训练速率\n",
    "                alpha = max(self.min_alpha, self.alpha * (1 - 1.0 * word_count[0] / total_words / self.epochs))\n",
    "                # 训练参数；统计当前job训练的词数，OOV的词不算\n",
    "                if self.sg >= 1:\n",
    "                    func = train_sentence_sg\n",
    "                else:\n",
    "                    func = train_sentence_cbow\n",
    "                job_words, total_loss = np.sum([func(self, sentence, alpha, work) for sentence in job], axis=0)\n",
    "                total_loss /= len(job)\n",
    "                # 请求锁，用来更新`word_count`\n",
    "                with lock:\n",
    "                    word_count[0] += job_words\n",
    "                    elapsed = time.time() - start\n",
    "                    # 防止一直打log\n",
    "                    if elapsed >= next_report[0]:\n",
    "                        logger.info(\"PROGRESS: at %.2f%% words, alpha %.05f, %.0f words/s, loss %.08f\" %\n",
    "                            (100.0 * word_count[0] / total_words / self.epochs, alpha, word_count[0] / elapsed if elapsed else 0.0, total_loss))\n",
    "                        # 可以让log保持一秒及一秒以上打一次\n",
    "                        next_report[0] = elapsed + 1.0\n",
    "\n",
    "        workers = [threading.Thread(target=worker_train) for _ in range(self.workers)]\n",
    "        for thread in workers:\n",
    "            thread.daemon = True  # 可以更便捷的用ctrl+c中断程序\n",
    "            thread.start()\n",
    "\n",
    "        # 把输入的string变成Vocab类，对于OOV则用None表示，并把数据拆分成多个job，存到queue里\n",
    "        no_oov = ([self.vocab.get(word, None) for word in sentence] for sentence in sentences)\n",
    "        for job_no, job in enumerate(grouper(no_oov, chunksize)):\n",
    "            logger.debug(\"putting job #%i in the queue, qsize=%i\" % (job_no, jobs.qsize()))\n",
    "            jobs.put(job)\n",
    "        logger.info(\"reached the end of input; waiting to finish %i outstanding jobs\" % jobs.qsize())\n",
    "        # 再补充`self.workers`个jobs，用来告知线程数据读取完成\n",
    "        for _ in range(self.workers):\n",
    "            jobs.put(None)\n",
    "\n",
    "        for thread in workers:\n",
    "            thread.join()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        logger.info(\"training on %i words took %.1fs, %.0f words/s\" %\n",
    "            (word_count[0], elapsed, word_count[0] / elapsed if elapsed else 0.0))\n",
    "\n",
    "        return word_count[0]\n",
    "\n",
    "    # ---辅助函数---\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\"\n",
    "        返回word对应的词向量\n",
    "        Example::\n",
    "          >>> trained_model['woman']\n",
    "          array([ -1.40128313e-02, ...]\n",
    "        \"\"\"\n",
    "        return self.syn0[self.vocab[word].index]\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        return word in self.vocab\n",
    "\n",
    "\n",
    "    def similarity(self, w1, w2):\n",
    "        \"\"\"\n",
    "        计算词w1与w2的余弦相似度\n",
    "        Example::\n",
    "          >>> trained_model.similarity('woman', 'man')\n",
    "          0.73723527\n",
    "          >>> trained_model.similarity('woman', 'woman')\n",
    "          1.0\n",
    "        \"\"\"\n",
    "        return np.dot(unitvec(self.syn0[w1]), unitvec(self.syn0[w2]))\n",
    "\n",
    "\n",
    "    def init_sims(self):\n",
    "        if getattr(self, 'syn0norm', None) is None:\n",
    "            logger.info(\"precomputing L2-norms of word weight vectors\")\n",
    "            self.syn0norm = np.vstack(unitvec(vec) for vec in self.syn0).astype(REAL)\n",
    "\n",
    "\n",
    "    def most_similar(self, positive=[], negative=[], topn=10):\n",
    "        \"\"\"\n",
    "        找到topn个最相似的词，即与postive最相似的且与negative最不相似的词\n",
    "        使用给定词的词向量余弦相似度的平均表示词类比\n",
    "\n",
    "        Example::\n",
    "          >>> model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "          [('queen', 0.50882536), ...]\n",
    "          >>> model.most_similar('dog')\n",
    "        \"\"\"\n",
    "        self.init_sims()\n",
    "        \n",
    "        if isinstance(positive, str) and not negative:\n",
    "            # allow calls like most_similar('dog'), as a shorthand for most_similar(['dog'])\n",
    "            positive = [positive]\n",
    "\n",
    "        # add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words\n",
    "        positive = [(word, 1.0) if isinstance(word, str) else word for word in positive]\n",
    "        negative = [(word, -1.0) if isinstance(word, str) else word for word in negative]\n",
    "\n",
    "        # compute the weighted average of all words\n",
    "        all_words, mean = set(), []\n",
    "        for word, weight in positive + negative:\n",
    "            if word in self.vocab:\n",
    "                mean.append(weight * unitvec(self.syn0[self.vocab[word].index]))\n",
    "                all_words.add(self.vocab[word].index)\n",
    "            else:\n",
    "                raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "        if not mean:\n",
    "            raise ValueError(\"cannot compute similarity with no input\")\n",
    "        mean = unitvec(np.asarray(mean).mean(axis=0)).astype(REAL)\n",
    "\n",
    "        dists = np.dot(self.syn0norm, mean)\n",
    "        if not topn:\n",
    "            return dists\n",
    "        best = np.argsort(dists)[::-1][:topn + len(all_words)]\n",
    "        # ignore (don't return) words from the input\n",
    "        result = [(self.id2word[sim], dists[sim]) for sim in best if sim not in all_words]\n",
    "        return result[:topn]\n",
    "\n",
    "\n",
    "    def accuracy(self, questions, restrict_vocab=30000):\n",
    "        \"\"\"\n",
    "        用于计算模型的准确率(内部评价方法)\n",
    "\n",
    "        `questions` 表示一个文件名，可以用\n",
    "        https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt for an example.\n",
    "        的数据进行测试。该文件内每行包含四元组的词，文件包含多个section，每个section分开测试，最终合成结果。\n",
    "\n",
    "        `restrict_vocab`用来筛选vocab中词频最高的词。\n",
    "        \"\"\"\n",
    "        ok_vocab = dict(sorted(self.vocab.iteritems(), key=lambda item: -item[1].count)[:restrict_vocab])\n",
    "        ok_index = set(v.index for v in ok_vocab.values())\n",
    "\n",
    "        # 计算内部评价的准确率\n",
    "        def log_accuracy(section):\n",
    "            correct, incorrect = section['correct'], section['incorrect']\n",
    "            if correct + incorrect > 0:\n",
    "                logger.info(\"%s: %.1f%% (%i/%i)\" %\n",
    "                    (section['section'], 100.0 * correct / (correct + incorrect),\n",
    "                    correct, correct + incorrect))\n",
    "\n",
    "        sections, section = [], None\n",
    "        for line_no, line in enumerate(open(questions)):\n",
    "            if line.startswith(': '):\n",
    "                # 把上一个section存起来，并计算该section的accuracy\n",
    "                if section:\n",
    "                    sections.append(section)\n",
    "                    log_accuracy(section)\n",
    "                section = {'section': line.lstrip(': ').strip(), 'correct': 0, 'incorrect': 0}\n",
    "            else:\n",
    "                if not section:\n",
    "                    raise ValueError(\"missing section header before line #%i in %s\" % (line_no, questions))\n",
    "                try:\n",
    "                    # 得到4个词\n",
    "                    a, b, c, expected = [word.lower() for word in line.split()]\n",
    "                except:\n",
    "                    logger.info(\"skipping invalid line #%i in %s\" % (line_no, questions))\n",
    "                # 去掉(a, b, c, expected)中包含OOV的元组\n",
    "                if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "                    logger.debug(\"skipping line #%i with OOV words: %s\" % (line_no, line))\n",
    "                    continue\n",
    "\n",
    "                ignore = set(self.vocab[v].index for v in [a, b, c])\n",
    "                predicted = None\n",
    "                # 找到positive为(b, c)且negative为a的最相似的词\n",
    "                # 如果该词为expected，则正确数累加一，否则错误数累加一\n",
    "                for index in np.argsort(self.most_similar(positive=[b, c], negative=[a], topn=False))[::-1]:\n",
    "                    if index in ok_index and index not in ignore:\n",
    "                        predicted = self.id2word[index]\n",
    "                        if predicted != expected:\n",
    "                            logger.debug(\"%s: expected %s, predicted %s\" % (line.strip(), expected, predicted))\n",
    "                        break\n",
    "                section['correct' if predicted == expected else 'incorrect'] += 1\n",
    "        if section:\n",
    "            # 保存最后一个section\n",
    "            sections.append(section)\n",
    "            log_accuracy(section)\n",
    "\n",
    "        total = {'section': 'total', 'correct': sum(s['correct'] for s in sections), 'incorrect': sum(s['incorrect'] for s in sections)}\n",
    "        log_accuracy(total)\n",
    "        sections.append(total)\n",
    "        return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-26 14:53:14,103 : MainThread : INFO : finished training model\n",
      "2020-04-26 14:53:14,108 : MainThread : INFO : built huffman tree with maximum node depth 5\n",
      "2020-04-26 14:53:14,110 : MainThread : INFO : training model with 2 workers on 19 vocabulary and 5 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey'], ['I', 'like', 'graph', 'and', 'stuff'], ['I', 'like', 'trees', 'and', 'stuff'], ['Sometimes', 'I', 'build', 'a', 'graph'], ['Sometimes', 'I', 'build', 'trees']]\n",
      "------------------------------------------------------------\n",
      "[<__main__.Vocab object at 0x7f8934a52828>, <__main__.Vocab object at 0x7f8934a524a8>, <__main__.Vocab object at 0x7f8934a52550>, <__main__.Vocab object at 0x7f8934a52588>, <__main__.Vocab object at 0x7f8934a52470>, <__main__.Vocab object at 0x7f8934a52518>, <__main__.Vocab object at 0x7f8934a52320>, <__main__.Vocab object at 0x7f8934a525c0>, <__main__.Vocab object at 0x7f8934a52780>, <__main__.Vocab object at 0x7f8934a52898>, <__main__.Vocab object at 0x7f8934a52a58>, <__main__.Vocab object at 0x7f8934a527b8>, <__main__.Vocab object at 0x7f8934a52860>, <__main__.Vocab object at 0x7f8934a52630>, <__main__.Vocab object at 0x7f8934a52b70>, <__main__.Vocab object at 0x7f8934a52940>, <__main__.Vocab object at 0x7f893435c860>, <__main__.Vocab object at 0x7f893435c198>, <__main__.Vocab object at 0x7f893435cc50>] <class 'list'>\n",
      "self.neg_labels >>: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "word_count: [0]\n",
      "total_words: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-26 14:53:15,143 : Thread-21 : INFO : PROGRESS: at 33.85% words, alpha 0.01731, 7878 words/s, loss 0.39531087\n",
      "2020-04-26 14:53:16,154 : Thread-21 : INFO : PROGRESS: at 70.76% words, alpha 0.00808, 8318 words/s, loss 0.37955376\n",
      "2020-04-26 14:53:16,695 : MainThread : INFO : reached the end of input; waiting to finish 4 outstanding jobs\n",
      "2020-04-26 14:53:16,942 : MainThread : INFO : training on 24000 words took 2.8s, 8480 words/s\n",
      "2020-04-26 14:53:16,943 : MainThread : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph similar: trees\n",
      "elapsed time:2.8379366397857666s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/envs/pt-tf-env/lib/python3.6/site-packages/ipykernel_launcher.py:519: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     if not os.path.exists('../data'):\n",
    "#         os.mkdir('../data')\n",
    "#     # 训练语料\n",
    "#     text8 = Text8Corpus('../data/text8.zip', sent_len=20, sent_num=200)\n",
    "#     # 训练模型\n",
    "#     logging.info(\"start training model\")\n",
    "\n",
    "    # skip-gram与negative sampling\n",
    "    # model = Word2Vec(sentences=text8, size=100, alpha=0.025,\n",
    "    #             window=3, min_count=5, seed=1, iters=1, \n",
    "    #             workers=1, min_alpha=0.0001, hs=0, sg=0, negative=10, \n",
    "    #             sort_vocab=True)\n",
    "\n",
    "    # cbow与negative sampling\n",
    "    # model = Word2Vec(sentences=text8, size=100, alpha=0.025, \n",
    "    # \t\t\t window=3, min_count=5, seed=1, iters=1, \n",
    "    # \t\t\t workers=1, min_alpha=0.0001, hs=0, sg=1, negative=10, \n",
    "    # \t\t\t sort_vocab=True)\n",
    "\n",
    "    # cbow与hierarchical softmax\n",
    "    # model = Word2Vec(sentences=text8, size=100, alpha=0.025, \n",
    "    # \t\t\t window=3, min_count=5, seed=1, iters=1, \n",
    "    # \t\t\t workers=1, min_alpha=0.0001, hs=1, sg=0, negative=10, \n",
    "    # \t\t\t sort_vocab=True)\n",
    "\n",
    "    # skip-gram与hierarchical softmax\n",
    "    # model = Word2Vec(sentences=text8, size=100, alpha=0.025, \n",
    "    # \t\t\t window=3, min_count=5, seed=1, iters=1, \n",
    "    # \t\t\t workers=1, min_alpha=0.0001, hs=1, sg=1, negative=10, \n",
    "    # \t\t\t sort_vocab=True)\n",
    "\n",
    "    logging.info(\"finished training model\")\n",
    "\n",
    "    # 简单例子测试\n",
    "    test_corpus = (\"\"\"human interface computer\n",
    "                    survey user computer system response time\n",
    "                    eps user interface system\n",
    "                    system human system eps\n",
    "                    user response time\n",
    "                    trees\n",
    "                    graph trees\n",
    "                    graph minors trees\n",
    "                    graph minors survey\n",
    "                    I like graph and stuff\n",
    "                    I like trees and stuff\n",
    "                    Sometimes I build a graph\n",
    "                    Sometimes I build trees\"\"\").split(\"\\n\")\n",
    "    start = time.time()\n",
    "    test_corpus = [_.split() for _ in test_corpus]\n",
    "    print(test_corpus)\n",
    "    print(\"---\"*20)\n",
    "    model = Word2Vec(sentences=test_corpus, size=5, alpha=0.025, \n",
    "                    window=5, min_count=1, seed=1, iters=500, \n",
    "                    workers=2, min_alpha=0.0001, hs=1, sg=1, negative=10, \n",
    "                    sort_vocab=True)\n",
    "    similar = model.most_similar('graph', topn=3)[0][0]\n",
    "    print(\"graph similar:\", similar)\n",
    "    print('elapsed time:{}s'.format(time.time() - start))\n",
    "    assert similar == 'trees'\n",
    "\n",
    "    # 测试模型，内部评价\n",
    "    # logging.info(\"start evaluate model\")\n",
    "    # model.accuracy('../data/questions-words.txt')\n",
    "    # logging.info(\"finished evaluate model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
